## En este documento se resumen las acciones para levantar todo lo visto hasta ahora. De manera 
## resumida:
1) Levantamos cluster, node group y fargate profile manual o con eksctl(recomendado).
   Si es con eksctl podemos especificar el addon de autoscaler (Video 60). Un ejemplo esta en 
   cluster_node_fargate.yaml
2) Si queremos cluster autoscaler, descargamos cluster-autoscaler-autodiscover.yaml (Video 60)
3) Instalamos metric server para poder hacer hpa (Video 59)
4) Levantamos el aws Load balancer ingress controller(Video 56)
5) Levantamos el external DNS ingress y crear la hosted zone (route53)(Video 57)
6) Levantar el CSI (si quieres volumen)(Video 58)
7) Crear el storageclass y el claim (si quieres volumen)(Video 58)
8) Levantar:
8.1) deployment con su servicio, en el deploy especificar volumenes.(Video 58)
8.2) HorizontalPodAutoscaler (hpa) (Video 59)
8.3) cluster-autoscaler-autodiscover(Video 60)
8.4) Objeto ingress

Comando para listar nuestros clusters:
aws eks list-clusters --region us-east-1
Actualizar la configuracion de kubectl para que interactue con el cluster mencionado, se crea
otro archivo config en carpeta .kube/:
aws eks update-kubeconfig --name cluster_name
Todos los charts descargados con helm:
helm ls -A
helm uninstall chart_name : Elimina el chart instalado
helm repo list : Lista de los repos agregados hasta ahora

A continuacion iremos punto por punto.

###########################################################################################
########################## --Punto 1: Levantar cluster-- ##################################
###########################################################################################
Antes de crear otro cluster, eliminar el archivo config de la carpeta .kube/ que esta en raiz
(dar cd en terminal para ir a la raiz)
El codigo es cluster_node_fargate.yaml. Comando de creacion:
eksctl create cluster -f archivo.yaml

###########################################################################################
######################## --Punto 2: Cluster autoscaler-- ##################################
###########################################################################################
Se define, a traves de manifest como va a ser nuestro cluster autoscaler, es decir, las reglas
para el escalado. Descargamos el manifest:

curl -o cluster-autoscaler-autodiscover.yaml https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml

Una vez descargado el archivo debes editar y reemplazar <YOUR CLUSTER NAME> por el nombre del
cluster(linea 165)

###########################################################################################
########################## --Punto 3: Metric Server-- ##################################
###########################################################################################
Instalamos metric server para definir la metrica de escalado de los pods.

Instalacion de metrics server en el namespace de kube-system:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

Revisamos la creacion del deployment y sus pods:
kubectl get deployment metrics-server -n kube-system 

Vemos logs de los pods de metrics-server
stern -n kube-system metrics-server

###########################################################################################
######################### --Punto 4: AWS LoadBalancer-- ##################################
###########################################################################################
Crear politica (nos paramos en la ruta de la carpeta Video_56 para que jale asi) y revisar
que la policita AWSLoadBalancerControllerIAMPolicy exista en IAM -> Policies:

aws iam create-policy \
    --policy-name AWSLoadBalancerControllerIAMPolicy \
    --policy-document file://iam_policy.json

Activar autenticacion oidc de nuestros pods con aws:
eksctl utils associate-iam-oidc-provider --cluster cluster_name --approve

Crear el rol necesario con la politica anterior. Verificar que en arn de la politica sea 
correcto y poner nombre correcto del cluster:

eksctl create iamserviceaccount \
  --cluster=mi-cluster \
  --namespace=kube-system \
  --name=aws-load-balancer-controller \
  --role-name "AmazonEKSLoadBalancerControllerRole" \
  --attach-policy-arn=arn:aws:iam::590184078061:policy/AWSLoadBalancerControllerIAMPolicy \
  --approve 

Liga para ver el comando de instalacion:
https://artifacthub.io/packages/helm/aws/aws-load-balancer-controller 
Agregar el repo a helm:
helm repo add eks https://aws.github.io/eks-charts

Instalar el aws Load balancer en el namespace kube-system:
helm install aws-load-balancer-controller eks/aws-load-balancer-controller --set clusterName=mi-cluster -n kube-system --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller

Si el namespace kube-system esta asociado a un 'nodo' de fargate, entonces debemos agregar la
vpcId al comando anterior. Este Id se puede ver en el stack de cloudFormation en la parte de 
resources y aparece como VPC:
helm install aws-load-balancer-controller eks/aws-load-balancer-controller --set clusterName=mi-cluster -n kube-system --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller --set vpcId=vpc-0df3a7e5e43ea49dd

###########################################################################################
###################### --Punto 5: ExternalDNS y Hosted Zone-- ##################################
###########################################################################################
Crear politica de IAM(Para que los pods de external DNS puedan listar zonas, RecordSets y
cambios sobre estos), parate en Video_57.

aws iam create-policy \
    --policy-name ExternalDNSrControllerIAMPolicy \
    --policy-document file://iam_policy.json

Hacer el mismo paso de autenticacion oidc en caso de que no este hecho aun:
eksctl utils associate-iam-oidc-provider --cluster cluster_name --approve

Crear el rol necesario con la politica anterior. Verificar que en arn de la politica sea 
correcto y poner nombre correcto del cluster:

eksctl create iamserviceaccount \
  --cluster mi-cluster \
  --namespace kube-system \
  --name external-dns \
  --role-name "ExternalDNSControllerRole" \
  --attach-policy-arn arn:aws:iam::590184078061:policy/ExternalDNSrControllerIAMPolicy \
  --approve

Agegar el repo en caso de que no este:
helm repo add bitnami https://charts.bitnami.com/bitnami

Instalamos externaldns en el namespace kube-system: 
helm install external-dns bitnami/external-dns -n kube-system --set serviceAccount.create=false --set serviceAccount.name=external-dns

Agregar vpcId en caso de que kube-system este en un 'nodo' fargate?:
helm install external-dns bitnami/external-dns -n kube-system --set serviceAccount.create=false --set serviceAccount.name=external-dns --set vpcId=vpc-0df3a7e5e43ea49dd

Revisamos los logs de los pods del load balancer y externaldns:
stern -n kube-system "aws-load-balancer|external-dns"

Crear hosted zone: Ir a Route53 -> hosted zones, crear nueva 
-> dar nombre (mi-diminio.com, por ejemplo) 
-> Type: Selecciona Public hosted zone si el dominio serÃ¡ accesible desde internet, o Private hosted zone si es para uso interno dentro de una VPC. 
-> Crear

###########################################################################################
########################## --Punto 6: Levantar CSI-- ##################################
###########################################################################################
Descargamos esta politica en json para crear, borrar volumenes, etc
curl -o example-iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/v0.9.0/docs/example-iam-policy.json

Creamos la politica en base al archivo anterior
aws iam create-policy --policy-name AmazonEKS_EBS_CSI_Driver_Policy --policy-document file://example-iam-policy.json

Creamos rol para nuestro driver CSI en base a la politica anterior:
eksctl create iamserviceaccount \
  --cluster=mi-cluster \
  --namespace=kube-system \
  --name=ebs-csi-controller-sa \
  --role-name "EKSEBSCSIDriverRole" \
  --attach-policy-arn=arn:aws:iam::590184078061:policy/AmazonEKS_EBS_CSI_Driver_Policy \
  --approve

Desplegar nuestro driver CSI en el namespace kube-system:
kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=master"

Revisar la cuenta de servicio para revisar que ya traiga la anotacion correspondiente con el 
arn del rol creado anteriormente (EKSEBSCSIDriverRole), el que tenga la anotacion correcta
deberia ser automatico:
kubectl describe sa -n kube-system ebs-csi-controller-sa

###########################################################################################
####################### --Punto 7: StorageClass y Claim-- ##################################
###########################################################################################
Levantar storageclass.yaml y claim.yaml

###########################################################################################
######################## --Punto 8: El resto-- ############################################
###########################################################################################
El resto de manifest a desplegar en orden son:
1) deployment.yaml
2) hpa.yaml
3) cluster-autoscaler-autodiscover.yaml
4) ingress.yaml Poner las subnets correctas, verlas en networks del cluster

Con el siguiente comando puedes ver si hay subnets que son de la misma zona de disponibilidad:
aws ec2 describe-subnets --subnet-ids subnet-078a3a100500150ca subnet-09ec1c3fca534769b
(ultimas dos ejemplos de subnets)

Con esto, se levanta un cluster con nodos ec2 y fargate(con opcion de escalado), se levantan 
el aws LoadBalancer, ExternalDNS, el driver CSI, metric server (para hpa), una hosted zone.
Y con los manifest el deploy y lo necesario para conectarse (aws lb y ExternalDNS), junto con
volumenes aprovisionados dinamicamente (CSI) y metodos de escalado de pods (hpa) y de nodos
(autoscaler) y su ingress para crear el LoadBalancer

###########################################################################################
########################## --Eliminacion-- ############################################
###########################################################################################
Eliminar lo hecho en los manifest:
kubectl delete -f ingress.yaml cluster-autoscaler.yaml hpa.yaml deployment.yaml storageclass.yaml claim.yaml

Eliminar metric-server:
kubectl delete deployment metrics-server -n kube-system
Eliminar el CSI: 
kubectl delete -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=master"
Eliminar externaldns:
helm uninstall external-dns -n kube-system
Eliminar aws lb:
helm uninstall aws-load-balancer-controller -n kube-system

Eliminar los roles:
eksctl delete iamserviceaccount --cluster=mi-cluster --namespace=kube-system --name=aws-load-balancer-controller
eksctl delete iamserviceaccount --cluster=mi-cluster --namespace=kube-system --name=external-dns
eksctl delete iamserviceaccount --cluster=mi-cluster --namespace=kube-system --name=ebs-csi-controller-sa

Eliminar cluster:
eksctl delete cluster -f archivo.yaml

Revisar en EC2, CloudFormation si hay algo que no sea default y eliminarlo

###########################################################################################
############################## --Notas-- ############################################
###########################################################################################
En caso de tener 2 o mas clusters es conveniente especificar que archivo config de la carpeta 
.kube/ le corresponde a cada uno. Supon que tienes el actual config asociado a un cluster con
version 1.26 de kubernetes, entonces podemos renombrar ese archivo con el comando:
mv config config-126
Si hacemos otro cluster con version 1.30 y ejecutamos comando de update-kubeconfig entonces
se creara otro archivo config, a este podemos de nuevo renombrarlo con:
mv config config-130
Ahora tenemos un cluster de cada version con su config correspondiente. Para alternar kubectl
entre los dos debemos especificarlo:

KUBECONFIG=/home/cesar/.kube/config-126 kubectl get all

Con kubectl api-versions podemos ver las ApiVersion de los objetos que se definen en los 
manifest, por ejemplo:
ApiVersion: v1
kind: Objeto

kubectl api-resources : Los recursos que maneja kubectl(deployments, pods, etc) y su respectiva
version

Para Actualizar cluster, ver Updating kubernetes version de la doc de aws eks. La actualizacion
se recomienda que sea de vesion en version, es decir, si estas en la 1.26 entonces actualiza a 
la 1.27, luego 1.28 y asi sucesivamente hasta llegar a la version deseada; no ir directo de 
la 1.26 a la 1.30 por ejemplo 

Otra nota: El addon Coredns solia aparecer como degraded, esto se puede deber a q su deploy
no tiene el numero de replicas que necesita funcionando. Para ver las replicas que necesita este
addon ejecuta:
kubectl get deployment coredns -n kube-system -o yaml
y fijate en las replicas. En este ejemplo el namespace kube-system (que es donde estan los addons)
se levanto en un nodo fargate, a lo mejor fargate en automatico escalo para que jalara bien.

Este comando nos da la zona de disponibilidad de un conjunto de subnets
aws ec2 describe-subnets --subnet-ids subnet-0f386fe89fee2cb3c subnet-07250d7d4371d5d98 --query 'Subnets[*].[SubnetId,AvailabilityZone]' --output table


